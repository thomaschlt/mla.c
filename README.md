# MLA (Multi-head Linear Attention) Implementation in C

A C implementation of Multi-head Linear Attention with RoPE (Rotary Position Embedding) support.

## Features

- Multi-head attention mechanism
- RoPE (Rotary Position Embedding) implementation
- Memory-efficient key-value caching
- Content and positional attention scoring
- Numerically stable softmax implementation
